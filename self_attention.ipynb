{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ioeTnNe9cv",
        "outputId": "b7add708-bcc7-44c3-c873-93a6df4f100a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.17813372 0.2423965  0.57946979]\n",
            " [0.01497999 0.13401896 0.85100104]\n",
            " [0.03379677 0.13755702 0.8286462 ]]\n",
            "Self-Attention Output:\n",
            " [[1.12174014 1.85978999 1.80745647 0.77517572]\n",
            " [1.2639278  2.07916732 1.92766231 0.83544167]\n",
            " [1.25675581 2.06554595 1.90458861 0.82646735]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  # Import NumPy for matrix operations\n",
        "\n",
        "# Step 1: Define Input (Word Embeddings)\n",
        "# Each row represents a word, each column represents a feature.\n",
        "# We have 3 words, each with 4 features.\n",
        "x = np.array([[1, 0, 1, 0],\n",
        "              [0, 2, 0, 2],\n",
        "              [1, 1, 1, 1]])\n",
        "\n",
        "# Step 2: Initialize Weight Matrices (Random Values)\n",
        "# These matrices transform input into Queries (Q), Keys (K), and Values (V)\n",
        "W_q = np.random.rand(4, 4)  # Query weight matrix (4x4)\n",
        "W_k = np.random.rand(4, 4)  # Key weight matrix (4x4)\n",
        "W_v = np.random.rand(4, 4)  # Value weight matrix (4x4)\n",
        "\n",
        "# Step 3: Compute Q, K, V Matrices\n",
        "# Q, K, V are computed by multiplying input (x) with respective weight matrices.\n",
        "Q = np.dot(x, W_q)  # Query matrix (3x4) = (3x4) * (4x4)\n",
        "K = np.dot(x, W_k)  # Key matrix (3x4) = (3x4) * (4x4)\n",
        "V = np.dot(x, W_v)  # Value matrix (3x4) = (3x4) * (4x4)\n",
        "\n",
        "# Step 4: Compute Attention Scores\n",
        "# Attention score formula: (QK^T) / sqrt(d_k)\n",
        "# (QK^T) measures similarity between queries and keys\n",
        "d_k = K.shape[-1]  # Number of features (d_k = 4 in this case)\n",
        "attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)  # (3x4) * (4x3) -> (3x3)\n",
        "\n",
        "# Step 5: Apply Softmax to Convert Scores to Probabilities\n",
        "# Softmax ensures attention weights sum to 1 (per row)\n",
        "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
        "\n",
        "# Step 6: Compute Weighted Sum of Values (Final Self-Attention Output)\n",
        "# Output = Attention Weights * Value Matrix (V)\n",
        "output = np.dot(attention_weights, V)  # (3x3) * (3x4) -> (3x4)\n",
        "\n",
        "# Step 7: Print Results\n",
        "print(\"Attention Weights:\\n\", attention_weights)  # Shows how much attention each word gives to others\n",
        "print(\"Self-Attention Output:\\n\", output)  # Final transformed word representations\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Import NumPy for matrix operations\n",
        "\n",
        "# Step 1: Define Input (Word Embeddings)\n",
        "# Each row represents a word, each column represents a feature.\n",
        "# We have 3 words, each with 4 features.\n",
        "x = np.array([[5, 0, 5, 0],\n",
        "              [0, 5, 0, 5],\n",
        "              [1, 1, 1, 1]])\n",
        "\n",
        "# Step 2: Initialize Weight Matrices (Random Values)\n",
        "# These matrices transform input into Queries (Q), Keys (K), and Values (V)\n",
        "W_q = np.random.rand(4, 4)  # Query weight matrix (4x4)\n",
        "W_k = np.random.rand(4, 4)  # Key weight matrix (4x4)\n",
        "W_v = np.random.rand(4, 4)  # Value weight matrix (4x4)\n",
        "\n",
        "# Step 3: Compute Q, K, V Matrices\n",
        "# Q, K, V are computed by multiplying input (x) with respective weight matrices.\n",
        "Q = np.dot(x, W_q)  # Query matrix (3x4) = (3x4) * (4x4)\n",
        "K = np.dot(x, W_k)  # Key matrix (3x4) = (3x4) * (4x4)\n",
        "V = np.dot(x, W_v)  # Value matrix (3x4) = (3x4) * (4x4)\n",
        "\n",
        "# Step 4: Compute Attention Scores\n",
        "# Attention score formula: (QK^T) / sqrt(d_k)\n",
        "# (QK^T) measures similarity between queries and keys\n",
        "d_k = K.shape[-1]  # Number of features (d_k = 4 in this case)\n",
        "attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)  # (3x4) * (4x3) -> (3x3)\n",
        "\n",
        "# Step 5: Apply Softmax to Convert Scores to Probabilities\n",
        "# Softmax ensures attention weights sum to 1 (per row)\n",
        "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
        "\n",
        "# Step 6: Compute Weighted Sum of Values (Final Self-Attention Output)\n",
        "# Output = Attention Weights * Value Matrix (V)\n",
        "output = np.dot(attention_weights, V)  # (3x3) * (3x4) -> (3x4)\n",
        "\n",
        "# Step 7: Print Results\n",
        "print(\"Attention Weights:\\n\", attention_weights)  # Shows how much attention each word gives to others\n",
        "print(\"Self-Attention Output:\\n\", output)  # Final transformed word representations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_LAL6UfAaLz",
        "outputId": "f2047021-c7f3-4e5a-e003-e9dbfcedb6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[9.74908457e-01 2.50915434e-02 1.24095377e-14]\n",
            " [9.99959407e-01 4.05927900e-05 8.63314987e-17]\n",
            " [9.40157001e-01 5.98420411e-02 9.58067525e-07]]\n",
            "Self-Attention Output:\n",
            " [[7.04376566 4.31312118 9.09001624 4.78743535]\n",
            " [7.04283851 4.41236383 9.15900864 4.80297518]\n",
            " [7.04504775 4.1754492  8.99430469 4.76587572]]\n"
          ]
        }
      ]
    }
  ]
}